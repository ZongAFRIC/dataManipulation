{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Description = open(\"exemple_texte.txt\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.TextIOWrapper name='exemple_texte.txt' mode='r' encoding='cp1252'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "De nos jours, l'explosion du volume et de la variété des données ne laisse plus planer de doute : le rôle du data scientist sera central dans les années à venir. Si vous êtes data scientist ou que vous souhaitez travailler avec Python, ce cours est pour vous. Omar Souissi, professeur associé en technologie de l'information et techniques d'optimisation, vous aide à acquérir les bases indispensables pour faire de la data science avec Python. Vous découvrirez comment utiliser deux bibliothèques indispensables, à savoir NumPy et Pandas. Vous verrez également quelques études de cas qui vous permettront d'assimiler facilement les différentes notions de Python pour l'analyse de données.\n"
     ]
    }
   ],
   "source": [
    "contenu =Description.read()\n",
    "print (contenu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['De',\n",
       " 'nos',\n",
       " 'jours,',\n",
       " \"l'explosion\",\n",
       " 'du',\n",
       " 'volume',\n",
       " 'et',\n",
       " 'de',\n",
       " 'la',\n",
       " 'variété',\n",
       " 'des',\n",
       " 'données',\n",
       " 'ne',\n",
       " 'laisse',\n",
       " 'plus',\n",
       " 'planer',\n",
       " 'de',\n",
       " 'doute',\n",
       " ':',\n",
       " 'le',\n",
       " 'rôle',\n",
       " 'du',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'sera',\n",
       " 'central',\n",
       " 'dans',\n",
       " 'les',\n",
       " 'années',\n",
       " 'à',\n",
       " 'venir.',\n",
       " 'Si',\n",
       " 'vous',\n",
       " 'êtes',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'ou',\n",
       " 'que',\n",
       " 'vous',\n",
       " 'souhaitez',\n",
       " 'travailler',\n",
       " 'avec',\n",
       " 'Python,',\n",
       " 'ce',\n",
       " 'cours',\n",
       " 'est',\n",
       " 'pour',\n",
       " 'vous.',\n",
       " 'Omar',\n",
       " 'Souissi,',\n",
       " 'professeur',\n",
       " 'associé',\n",
       " 'en',\n",
       " 'technologie',\n",
       " 'de',\n",
       " \"l'information\",\n",
       " 'et',\n",
       " 'techniques',\n",
       " \"d'optimisation,\",\n",
       " 'vous',\n",
       " 'aide',\n",
       " 'à',\n",
       " 'acquérir',\n",
       " 'les',\n",
       " 'bases',\n",
       " 'indispensables',\n",
       " 'pour',\n",
       " 'faire',\n",
       " 'de',\n",
       " 'la',\n",
       " 'data',\n",
       " 'science',\n",
       " 'avec',\n",
       " 'Python.',\n",
       " 'Vous',\n",
       " 'découvrirez',\n",
       " 'comment',\n",
       " 'utiliser',\n",
       " 'deux',\n",
       " 'bibliothèques',\n",
       " 'indispensables,',\n",
       " 'à',\n",
       " 'savoir',\n",
       " 'NumPy',\n",
       " 'et',\n",
       " 'Pandas.',\n",
       " 'Vous',\n",
       " 'verrez',\n",
       " 'également',\n",
       " 'quelques',\n",
       " 'études',\n",
       " 'de',\n",
       " 'cas',\n",
       " 'qui',\n",
       " 'vous',\n",
       " 'permettront',\n",
       " \"d'assimiler\",\n",
       " 'facilement',\n",
       " 'les',\n",
       " 'différentes',\n",
       " 'notions',\n",
       " 'de',\n",
       " 'Python',\n",
       " 'pour',\n",
       " \"l'analyse\",\n",
       " 'de',\n",
       " 'données.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = contenu.split(\" \")\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\booth15-mgr2/nltk_data'\n    - 'C:\\\\Users\\\\booth15-mgr2\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\booth15-mgr2\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\booth15-mgr2\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\booth15-mgr2\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-f29e53fbc2b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontenu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \"\"\"\n\u001b[1;32m--> 144\u001b[1;33m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m     return [\n\u001b[0;32m    146\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \"\"\"\n\u001b[1;32m--> 105\u001b[1;33m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    866\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    867\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 868\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'raw'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'nltk'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 993\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    994\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'file'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    995\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    699\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'*'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\booth15-mgr2/nltk_data'\n    - 'C:\\\\Users\\\\booth15-mgr2\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\booth15-mgr2\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\booth15-mgr2\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\booth15-mgr2\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "nltk.word_tokenize(contenu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\booth15-mgr2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['De',\n",
       " 'nos',\n",
       " 'jours',\n",
       " ',',\n",
       " \"l'explosion\",\n",
       " 'du',\n",
       " 'volume',\n",
       " 'et',\n",
       " 'de',\n",
       " 'la',\n",
       " 'variété',\n",
       " 'des',\n",
       " 'données',\n",
       " 'ne',\n",
       " 'laisse',\n",
       " 'plus',\n",
       " 'planer',\n",
       " 'de',\n",
       " 'doute',\n",
       " ':',\n",
       " 'le',\n",
       " 'rôle',\n",
       " 'du',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'sera',\n",
       " 'central',\n",
       " 'dans',\n",
       " 'les',\n",
       " 'années',\n",
       " 'à',\n",
       " 'venir',\n",
       " '.',\n",
       " 'Si',\n",
       " 'vous',\n",
       " 'êtes',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'ou',\n",
       " 'que',\n",
       " 'vous',\n",
       " 'souhaitez',\n",
       " 'travailler',\n",
       " 'avec',\n",
       " 'Python',\n",
       " ',',\n",
       " 'ce',\n",
       " 'cours',\n",
       " 'est',\n",
       " 'pour',\n",
       " 'vous',\n",
       " '.',\n",
       " 'Omar',\n",
       " 'Souissi',\n",
       " ',',\n",
       " 'professeur',\n",
       " 'associé',\n",
       " 'en',\n",
       " 'technologie',\n",
       " 'de',\n",
       " \"l'information\",\n",
       " 'et',\n",
       " 'techniques',\n",
       " \"d'optimisation\",\n",
       " ',',\n",
       " 'vous',\n",
       " 'aide',\n",
       " 'à',\n",
       " 'acquérir',\n",
       " 'les',\n",
       " 'bases',\n",
       " 'indispensables',\n",
       " 'pour',\n",
       " 'faire',\n",
       " 'de',\n",
       " 'la',\n",
       " 'data',\n",
       " 'science',\n",
       " 'avec',\n",
       " 'Python',\n",
       " '.',\n",
       " 'Vous',\n",
       " 'découvrirez',\n",
       " 'comment',\n",
       " 'utiliser',\n",
       " 'deux',\n",
       " 'bibliothèques',\n",
       " 'indispensables',\n",
       " ',',\n",
       " 'à',\n",
       " 'savoir',\n",
       " 'NumPy',\n",
       " 'et',\n",
       " 'Pandas',\n",
       " '.',\n",
       " 'Vous',\n",
       " 'verrez',\n",
       " 'également',\n",
       " 'quelques',\n",
       " 'études',\n",
       " 'de',\n",
       " 'cas',\n",
       " 'qui',\n",
       " 'vous',\n",
       " 'permettront',\n",
       " \"d'assimiler\",\n",
       " 'facilement',\n",
       " 'les',\n",
       " 'différentes',\n",
       " 'notions',\n",
       " 'de',\n",
       " 'Python',\n",
       " 'pour',\n",
       " \"l'analyse\",\n",
       " 'de',\n",
       " 'données',\n",
       " '.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(contenu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['De',\n",
       " 'nos',\n",
       " 'jours',\n",
       " 'l',\n",
       " 'explosion',\n",
       " 'du',\n",
       " 'volume',\n",
       " 'et',\n",
       " 'de',\n",
       " 'la',\n",
       " 'variété',\n",
       " 'des',\n",
       " 'données',\n",
       " 'ne',\n",
       " 'laisse',\n",
       " 'plus',\n",
       " 'planer',\n",
       " 'de',\n",
       " 'doute',\n",
       " 'le',\n",
       " 'rôle',\n",
       " 'du',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'sera',\n",
       " 'central',\n",
       " 'dans',\n",
       " 'les',\n",
       " 'années',\n",
       " 'à',\n",
       " 'venir',\n",
       " 'Si',\n",
       " 'vous',\n",
       " 'êtes',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'ou',\n",
       " 'que',\n",
       " 'vous',\n",
       " 'souhaitez',\n",
       " 'travailler',\n",
       " 'avec',\n",
       " 'Python',\n",
       " 'ce',\n",
       " 'cours',\n",
       " 'est',\n",
       " 'pour',\n",
       " 'vous',\n",
       " 'Omar',\n",
       " 'Souissi',\n",
       " 'professeur',\n",
       " 'associé',\n",
       " 'en',\n",
       " 'technologie',\n",
       " 'de',\n",
       " 'l',\n",
       " 'information',\n",
       " 'et',\n",
       " 'techniques',\n",
       " 'd',\n",
       " 'optimisation',\n",
       " 'vous',\n",
       " 'aide',\n",
       " 'à',\n",
       " 'acquérir',\n",
       " 'les',\n",
       " 'bases',\n",
       " 'indispensables',\n",
       " 'pour',\n",
       " 'faire',\n",
       " 'de',\n",
       " 'la',\n",
       " 'data',\n",
       " 'science',\n",
       " 'avec',\n",
       " 'Python',\n",
       " 'Vous',\n",
       " 'découvrirez',\n",
       " 'comment',\n",
       " 'utiliser',\n",
       " 'deux',\n",
       " 'bibliothèques',\n",
       " 'indispensables',\n",
       " 'à',\n",
       " 'savoir',\n",
       " 'NumPy',\n",
       " 'et',\n",
       " 'Pandas',\n",
       " 'Vous',\n",
       " 'verrez',\n",
       " 'également',\n",
       " 'quelques',\n",
       " 'études',\n",
       " 'de',\n",
       " 'cas',\n",
       " 'qui',\n",
       " 'vous',\n",
       " 'permettront',\n",
       " 'd',\n",
       " 'assimiler',\n",
       " 'facilement',\n",
       " 'les',\n",
       " 'différentes',\n",
       " 'notions',\n",
       " 'de',\n",
       " 'Python',\n",
       " 'pour',\n",
       " 'l',\n",
       " 'analyse',\n",
       " 'de',\n",
       " 'données']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "Sac_de_mots= tokenizer.tokenize(contenu)\n",
    "Sac_de_mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\booth15-mgr2/nltk_data'\n    - 'C:\\\\Users\\\\booth15-mgr2\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\booth15-mgr2\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\booth15-mgr2\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\booth15-mgr2\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\booth15-mgr2/nltk_data'\n    - 'C:\\\\Users\\\\booth15-mgr2\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\booth15-mgr2\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\booth15-mgr2\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\booth15-mgr2\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-0b54a6aa9ea3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstopwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'french'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     86\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m                 \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    699\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'*'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\booth15-mgr2/nltk_data'\n    - 'C:\\\\Users\\\\booth15-mgr2\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\booth15-mgr2\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\booth15-mgr2\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\booth15-mgr2\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('french')\n",
    "stopwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\booth15-mgr2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('french')\n",
    "stopwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filtre=[]\n",
    "for w in Sac_de_mots[1:]: \n",
    "    if w not in stopwords: \n",
    "        Filtre.append(w) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jours',\n",
       " 'explosion',\n",
       " 'volume',\n",
       " 'variété',\n",
       " 'données',\n",
       " 'laisse',\n",
       " 'plus',\n",
       " 'planer',\n",
       " 'doute',\n",
       " 'rôle',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'central',\n",
       " 'années',\n",
       " 'venir',\n",
       " 'Si',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'souhaitez',\n",
       " 'travailler',\n",
       " 'Python',\n",
       " 'cours',\n",
       " 'Omar',\n",
       " 'Souissi',\n",
       " 'professeur',\n",
       " 'associé',\n",
       " 'technologie',\n",
       " 'information',\n",
       " 'techniques',\n",
       " 'optimisation',\n",
       " 'aide',\n",
       " 'acquérir',\n",
       " 'bases',\n",
       " 'indispensables',\n",
       " 'faire',\n",
       " 'data',\n",
       " 'science',\n",
       " 'Python',\n",
       " 'Vous',\n",
       " 'découvrirez',\n",
       " 'comment',\n",
       " 'utiliser',\n",
       " 'deux',\n",
       " 'bibliothèques',\n",
       " 'indispensables',\n",
       " 'savoir',\n",
       " 'NumPy',\n",
       " 'Pandas',\n",
       " 'Vous',\n",
       " 'verrez',\n",
       " 'également',\n",
       " 'quelques',\n",
       " 'études',\n",
       " 'cas',\n",
       " 'permettront',\n",
       " 'assimiler',\n",
       " 'facilement',\n",
       " 'différentes',\n",
       " 'notions',\n",
       " 'Python',\n",
       " 'analyse',\n",
       " 'données']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Filtre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Filtre.count(\"Python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
